{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding , Dropout\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner(BaseEstimator, TransformerMixin):    \n",
    "    def remove_mentions(self, text):        \n",
    "        return re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    def remove_urls(self, text):        \n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', text)\n",
    "    \n",
    "    def only_characters(self, text):\n",
    "        return re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    def remove_extra_spaces(self, text):\n",
    "        text = re.sub(\"\\s+\", ' ', text)\n",
    "        text = text.lstrip()\n",
    "        return text.rstrip()\n",
    "    \n",
    "    def to_lower(self, text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def fix_words(self, text):\n",
    "        text = re.sub(r'\\bthx\\b', 'thanks', text)\n",
    "        text = re.sub(r'\\bu\\b', 'you', text)\n",
    "        text = re.sub(r'\\bhrs\\b', 'hours', text)\n",
    "        text = re.sub(r'\\baa\\b', 'a', text)\n",
    "        text = re.sub(r'\\bflightr\\b', 'flight', text)\n",
    "        text = re.sub(r'\\bur\\b', 'your', text)\n",
    "        text = re.sub(r'\\bhr\\b', 'hour', text)\n",
    "        text = re.sub(r'\\bthru\\b', 'through', text)\n",
    "        text = re.sub(r'\\br\\b', 'are', text)\n",
    "        text = re.sub(r'\\bppl\\b', 'people', text)\n",
    "        text = re.sub(r'\\btix\\b', 'fix', text)\n",
    "        text = re.sub(r'\\bplz\\b', 'please', text)\n",
    "        text = re.sub(r'\\bflightd\\b', 'flighted', text)\n",
    "        text = re.sub(r'\\btmrw\\b', 'tomorrow', text)\n",
    "        text = re.sub(r'\\bthx\\b', 'thanks', text)\n",
    "        text = re.sub(r'\\bpls\\b', 'please', text)\n",
    "        text = re.sub(r'\\bfyi\\b', 'for your information', text)\n",
    "        \n",
    "        text = re.sub(r'\\bheyyyy\\b', 'hey', text)\n",
    "        text = re.sub(r'\\bguyyyys\\b', 'guys', text)\n",
    "        text = re.sub(r'\\byall\\b', 'you all', text)\n",
    "        text = re.sub(r'\\basap\\b', 'as soon as possible', text)\n",
    "        text = re.sub(r'\\bbtw\\b', 'by the way', text)\n",
    "        text = re.sub(r'\\bdm\\b', 'direct message', text)\n",
    "        text = re.sub(r'\\bcudtomers\\b', 'customers', text)\n",
    "        text = re.sub(r'\\bwtf\\b', 'what the fuck', text)\n",
    "        text = re.sub(r'\\biphone\\b', 'phone', text)\n",
    "        text = re.sub(r'\\bmins\\b', 'minutes', text)\n",
    "        text = re.sub(r'\\btv\\b', 'television', text)\n",
    "        text = re.sub(r'\\bokay\\b', 'ok', text)\n",
    "        text = re.sub(r'\\bfeb\\b', 'february', text)\n",
    "        text = re.sub(r'\\byr\\b', 'year', text)\n",
    "        text = re.sub(r'\\bshes\\b', 'she is', text)\n",
    "        text = re.sub(r'\\bnope\\b', 'no', text)\n",
    "        text = re.sub(r'\\bhes\\b', 'he is', text)\n",
    "        text = re.sub(r'\\btill\\b', 'until', text)\n",
    "        text = re.sub(r'\\bomg\\b', 'oh my god', text)\n",
    "        text = re.sub(r'\\btho\\b', 'though', text)\n",
    "        text = re.sub(r'\\bnothappy\\b', 'not happy', text)\n",
    "        return re.sub(r'\\bthankyou\\b', 'thank you', text)\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):        \n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.only_characters).apply(self.remove_extra_spaces).apply(self.to_lower).apply(self.fix_words)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'Task2')\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 40000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dfXY  = pd.read_csv('labels_titles.csv')\n",
    "df=pd.read_csv('Airline-Sentiment-2-w-AA.csv', encoding='ISO-8859-1',usecols=['text', 'airline_sentiment'])\n",
    "#X_test, Y_test = read_csv('data/tesss.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv('Task2/orig.csv',encoding = \"ISO-8859-1\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,1]\n",
    "y=df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['target'] = le.fit_transform(df['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    1\n",
       "3    0\n",
       "4    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TextCleaner()\n",
    "df['clean_text'] = tc.transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['clean_text']\n",
    "y=df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = np.array(dfXY.iloc[:,1]).astype(str)\n",
    "y = np.array(dfXY.iloc[:,2]).astype(int)\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "itemindex1 = np.where(y<0)\n",
    "for index in itemindex1:\n",
    "    print(index)\n",
    "    y[index]=np.random.randint(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 179, 93, 743, 534, 3, 71, 1, 141, 136, 183]\n",
      "Found 12523 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)                                #num_words basically tells the size of vocabulary.\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "print(sequences[2])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (14640, 100)\n",
      "Shape of label tensor: (14640, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(y))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100, 100)\n",
      "(?, 98, 128)\n",
      "(?, 32, 128)\n",
      "(?, 30, 128)\n",
      "(?, 10, 128)\n",
      "(?, 6, 128)\n",
      "(?, 128)\n",
      "(?, 128)\n",
      "(?, 3)\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "print(embedded_sequences.shape)\n",
    "x = Conv1D(128, 3, activation='relu')(embedded_sequences)\n",
    "print(x.shape)\n",
    "x = MaxPooling1D(3)(x)\n",
    "print(x.shape)\n",
    "x=Dropout(0.5)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "print(x.shape)\n",
    "x = MaxPooling1D(3)(x)\n",
    "print(x.shape)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "print(x.shape)\n",
    "x=Dropout(0.5)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "print(x.shape)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "print(x.shape)\n",
    "preds = Dense(3, activation='softmax')(x)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13176 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "13176/13176 [==============================] - 8s 612us/step - loss: 0.3877 - acc: 0.8434 - val_loss: 0.5221 - val_acc: 0.7951\n",
      "Epoch 2/30\n",
      "13176/13176 [==============================] - 7s 555us/step - loss: 0.3578 - acc: 0.8582 - val_loss: 0.5442 - val_acc: 0.7883\n",
      "Epoch 3/30\n",
      "13176/13176 [==============================] - 7s 561us/step - loss: 0.3309 - acc: 0.8659 - val_loss: 0.5642 - val_acc: 0.7801\n",
      "Epoch 4/30\n",
      "13176/13176 [==============================] - 7s 556us/step - loss: 0.3240 - acc: 0.8711 - val_loss: 0.5319 - val_acc: 0.7862\n",
      "Epoch 5/30\n",
      "13176/13176 [==============================] - 7s 554us/step - loss: 0.3072 - acc: 0.8749 - val_loss: 0.5385 - val_acc: 0.7876\n",
      "Epoch 6/30\n",
      "13176/13176 [==============================] - 7s 558us/step - loss: 0.2913 - acc: 0.8864 - val_loss: 0.5476 - val_acc: 0.7842\n",
      "Epoch 7/30\n",
      "13176/13176 [==============================] - 7s 560us/step - loss: 0.2706 - acc: 0.8906 - val_loss: 0.5594 - val_acc: 0.7889\n",
      "Epoch 8/30\n",
      "13176/13176 [==============================] - 7s 562us/step - loss: 0.2652 - acc: 0.8925 - val_loss: 0.6128 - val_acc: 0.7719\n",
      "Epoch 9/30\n",
      "13176/13176 [==============================] - 7s 568us/step - loss: 0.2510 - acc: 0.8977 - val_loss: 0.6263 - val_acc: 0.7582\n",
      "Epoch 10/30\n",
      "13176/13176 [==============================] - 7s 568us/step - loss: 0.2452 - acc: 0.9044 - val_loss: 0.6949 - val_acc: 0.7691\n",
      "Epoch 11/30\n",
      "13176/13176 [==============================] - 7s 566us/step - loss: 0.2285 - acc: 0.9095 - val_loss: 0.6352 - val_acc: 0.7835\n",
      "Epoch 12/30\n",
      "13176/13176 [==============================] - 7s 565us/step - loss: 0.2300 - acc: 0.9086 - val_loss: 0.6922 - val_acc: 0.7794\n",
      "Epoch 13/30\n",
      "13176/13176 [==============================] - 7s 566us/step - loss: 0.2137 - acc: 0.9146 - val_loss: 0.7603 - val_acc: 0.7643\n",
      "Epoch 14/30\n",
      "13176/13176 [==============================] - 7s 564us/step - loss: 0.2006 - acc: 0.9189 - val_loss: 0.6869 - val_acc: 0.7664\n",
      "Epoch 15/30\n",
      "13176/13176 [==============================] - 7s 562us/step - loss: 0.2005 - acc: 0.9205 - val_loss: 0.7219 - val_acc: 0.7753\n",
      "Epoch 16/30\n",
      "13176/13176 [==============================] - 8s 570us/step - loss: 0.1880 - acc: 0.9246 - val_loss: 0.7499 - val_acc: 0.7623\n",
      "Epoch 17/30\n",
      "13176/13176 [==============================] - 7s 567us/step - loss: 0.1898 - acc: 0.9253 - val_loss: 0.7619 - val_acc: 0.7623\n",
      "Epoch 18/30\n",
      "13176/13176 [==============================] - 7s 566us/step - loss: 0.1842 - acc: 0.9261 - val_loss: 0.8388 - val_acc: 0.7527\n",
      "Epoch 19/30\n",
      "13176/13176 [==============================] - 7s 562us/step - loss: 0.1752 - acc: 0.9298 - val_loss: 0.8081 - val_acc: 0.7637\n",
      "Epoch 20/30\n",
      "13176/13176 [==============================] - 7s 565us/step - loss: 0.1716 - acc: 0.9303 - val_loss: 0.8650 - val_acc: 0.7575\n",
      "Epoch 21/30\n",
      "13176/13176 [==============================] - 7s 564us/step - loss: 0.1710 - acc: 0.9325 - val_loss: 0.8831 - val_acc: 0.7609\n",
      "Epoch 22/30\n",
      "13176/13176 [==============================] - 7s 564us/step - loss: 0.1569 - acc: 0.9368 - val_loss: 0.8576 - val_acc: 0.7575\n",
      "Epoch 23/30\n",
      "13176/13176 [==============================] - 7s 561us/step - loss: 0.1574 - acc: 0.9365 - val_loss: 0.8409 - val_acc: 0.7568\n",
      "Epoch 24/30\n",
      "13176/13176 [==============================] - 7s 561us/step - loss: 0.1575 - acc: 0.9383 - val_loss: 0.8607 - val_acc: 0.7630\n",
      "Epoch 25/30\n",
      "13176/13176 [==============================] - 7s 563us/step - loss: 0.1513 - acc: 0.9404 - val_loss: 0.9979 - val_acc: 0.7541\n",
      "Epoch 26/30\n",
      "13176/13176 [==============================] - 7s 566us/step - loss: 0.1494 - acc: 0.9414 - val_loss: 0.8900 - val_acc: 0.7486\n",
      "Epoch 27/30\n",
      "13176/13176 [==============================] - 7s 561us/step - loss: 0.1456 - acc: 0.9429 - val_loss: 0.9205 - val_acc: 0.7671\n",
      "Epoch 28/30\n",
      "13176/13176 [==============================] - 7s 562us/step - loss: 0.1412 - acc: 0.9446 - val_loss: 0.9872 - val_acc: 0.7493\n",
      "Epoch 29/30\n",
      "13176/13176 [==============================] - 7s 559us/step - loss: 0.1382 - acc: 0.9456 - val_loss: 1.0130 - val_acc: 0.7391\n",
      "Epoch 30/30\n",
      "13176/13176 [==============================] - 7s 559us/step - loss: 0.1366 - acc: 0.9466 - val_loss: 1.0359 - val_acc: 0.7705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd9dcecf98>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=30,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
